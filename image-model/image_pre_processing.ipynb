{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d88d6e",
   "metadata": {},
   "source": [
    "# Image Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b98638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lock-n-key\r\n",
      "lock-pick\r\n",
      "empty\r\n"
     ]
    }
   ],
   "source": [
    "# Create the necessary directories\n",
    "!for cat in $(echo 'lock-n-key lock-pick empty'); do \\\n",
    "    echo $cat; mkdir -p \"datasets/train/$cat\" \"datasets/validate/$cat\" \"datasets/test/$cat\"; \\\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "238557da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyheif in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (0.5.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from pyheif) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from cffi>=1.0.0->pyheif) (2.20)\n",
      "Requirement already satisfied: tensorflow in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (2.6.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: six~=1.15.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: keras~=2.6 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: gast==0.4.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (1.39.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: clang~=5.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (53.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.4.5)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/dorcas/Documents/NUS/Modules/CS3237/venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyheif\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d1c38",
   "metadata": {},
   "source": [
    "## Image Conversion & Resizing\n",
    "\n",
    "Images in Apple's HEIC format need to be converted into JPEG form in order to be read by standard Python libraries. \n",
    "\n",
    "Rectangular images are also resized into squares. For simplicity, they are scaled instead of cropped, meaning that this operation could also be carried out by `ImageDataGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6ce9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12900fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, unique\n",
    "import os\n",
    "from PIL import Image\n",
    "import pyheif\n",
    "import random\n",
    "\n",
    "@unique\n",
    "class ImageFormat(Enum):\n",
    "    HEIF = 'HEIF'\n",
    "    JPEG = 'JPEG'\n",
    "    PNG = 'PNG'\n",
    "    OTHER = ''\n",
    "\n",
    "    @classmethod\n",
    "    def from_ext(self, ext):\n",
    "        ext = ext.lower()\n",
    "        if ext in self._ext_map.keys():\n",
    "            return self._ext_map[ext]\n",
    "        else:\n",
    "            return self.OTHER\n",
    "\n",
    "ImageFormat._ext_map = {\n",
    "    '.heic': ImageFormat.HEIF,\n",
    "    '.jpg': ImageFormat.JPEG,\n",
    "    '.jpeg': ImageFormat.JPEG,\n",
    "    '.png': ImageFormat.PNG,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc17853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(s, prefix):\n",
    "    return s[len(prefix):] if s.startswith(prefix) else s\n",
    "\n",
    "INPUT_DIR = 'tmp/Data Collection (Images)'\n",
    "\n",
    "OUTPUT_DIR = 'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4374dee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 608 files in tmp/Data Collection (Images)/Empty/Dorcas-final...\n",
      "Processed 107 files in tmp/Data Collection (Images)/Empty/Dorcas...\n",
      "Processed 125 files in tmp/Data Collection (Images)/Empty/Steve...\n",
      "Processed 105 files in tmp/Data Collection (Images)/Empty/An Qi...\n",
      "Processed 30 files in tmp/Data Collection (Images)/Empty/Lucas...\n",
      "Processed 0 files in tmp/Data Collection (Images)/Empty...\n",
      "Processed 561 files in tmp/Data Collection (Images)/Lock + Key/Dorcas-final...\n",
      "Processed 102 files in tmp/Data Collection (Images)/Lock + Key/Dorcas...\n",
      "Processed 66 files in tmp/Data Collection (Images)/Lock + Key/Steve...\n",
      "Processed 104 files in tmp/Data Collection (Images)/Lock + Key/An Qi...\n",
      "Processed 83 files in tmp/Data Collection (Images)/Lock + Key/Lucas...\n",
      "Processed 0 files in tmp/Data Collection (Images)/Lock + Key...\n",
      "Processed 439 files in tmp/Data Collection (Images)/Lock Picking/Dorcas-final...\n",
      "Processed 60 files in tmp/Data Collection (Images)/Lock Picking/Rectangular...\n",
      "Processed 110 files in tmp/Data Collection (Images)/Lock Picking/Dorcas...\n",
      "Processed 80 files in tmp/Data Collection (Images)/Lock Picking/Steve...\n",
      "Processed 76 files in tmp/Data Collection (Images)/Lock Picking/An Qi...\n",
      "Processed 165 files in tmp/Data Collection (Images)/Lock Picking/Lucas...\n",
      "Processed 0 files in tmp/Data Collection (Images)/Lock Picking...\n",
      "Processed 0 files in tmp/Data Collection (Images)...\n"
     ]
    }
   ],
   "source": [
    "num_images_per_class = 1000\n",
    "\n",
    "validation_fraction = 0.2\n",
    "test_fraction = 0.2\n",
    "training_fraction = 1 - test_fraction - validation_fraction\n",
    "\n",
    "# TODO: reduce hardcoding\n",
    "\n",
    "class_map = {\n",
    "    '/Lock Picking/An Qi': 'lock-pick',\n",
    "    '/Lock Picking/Dorcas': 'lock-pick',\n",
    "    '/Lock Picking/Dorcas-final': 'lock-pick',\n",
    "    '/Lock Picking/Lucas': 'lock-pick',\n",
    "    '/Lock Picking/Rectangular': 'lock-pick',\n",
    "    '/Lock Picking/Steve': 'lock-pick',\n",
    "    '/Lock + Key/An Qi': 'lock-n-key',\n",
    "    '/Lock + Key/Dorcas': 'lock-n-key',\n",
    "    '/Lock + Key/Dorcas-final': 'lock-n-key',\n",
    "    '/Lock + Key/Lucas': 'lock-n-key',\n",
    "    '/Lock + Key/Steve': 'lock-n-key',\n",
    "    '/Empty/An Qi': 'empty',\n",
    "    '/Empty/Dorcas': 'empty',\n",
    "    '/Empty/Dorcas-final': 'empty',\n",
    "    '/Empty/Lucas': 'empty',\n",
    "    '/Empty/Steve': 'empty',\n",
    "}\n",
    "\n",
    "def open_heif_image(filepath):\n",
    "    heif_file = pyheif.read(filepath)\n",
    "    image = Image.frombytes(\n",
    "        heif_file.mode,\n",
    "        heif_file.size,\n",
    "        heif_file.data,\n",
    "        \"raw\",\n",
    "        heif_file.mode,\n",
    "        heif_file.stride,\n",
    "    )\n",
    "    return image\n",
    "\n",
    "\n",
    "def open_image(filepath):\n",
    "    return Image.open(filepath)\n",
    "\n",
    "# TODO: better abstraction and fewer global variables\n",
    "\n",
    "datasets = ['train', 'test', 'validate']\n",
    "dataset_counts = {\n",
    "    'lock-pick': [600, 200, 200], \n",
    "    'lock-n-key': [600, 200, 200], \n",
    "    'empty': [600, 200, 200]\n",
    "}\n",
    "\n",
    "def weighted_random_choice():\n",
    "    return random.choices([0, 1, 2], [600, 200, 200])[0]\n",
    "\n",
    "\n",
    "def get_random_dataset(class_name):\n",
    "    counts = dataset_counts[class_name]\n",
    "    rand_dataset = weighted_random_choice()\n",
    "    while counts[rand_dataset] <= 0:\n",
    "        rand_dataset = weighted_random_choice()\n",
    "    counts[rand_dataset] -= 1\n",
    "    return datasets[rand_dataset]\n",
    "\n",
    "\n",
    "def get_output_path(orig_path):\n",
    "    relative_path = remove_prefix(orig_path, INPUT_DIR)\n",
    "    class_name = class_map[relative_path]\n",
    "    dataset_name = get_random_dataset(class_name)\n",
    "    return \"%s/%s/%s\" % (OUTPUT_DIR, dataset_name, class_name)\n",
    "\n",
    "\n",
    "def resize_and_save(image, path):\n",
    "    resized_image = image.resize(target_size)\n",
    "    resized_image.save(path)\n",
    "\n",
    "\n",
    "def process_file(path, filename):\n",
    "    filepath = os.path.join(path, filename)\n",
    "    # print(\"Processing %s...\" % filepath)\n",
    "\n",
    "    (raw_filename, ext) = os.path.splitext(filename)\n",
    "\n",
    "    image_fmt = ImageFormat.from_ext(ext)\n",
    "    if image_fmt == ImageFormat.HEIF:\n",
    "        image = open_heif_image(filepath)\n",
    "    else:\n",
    "        image = open_image(filepath)\n",
    "\n",
    "    output_filepath = \"./%s/%s.jpg\" % (get_output_path(path), raw_filename)\n",
    "    resize_and_save(image, output_filepath)\n",
    "\n",
    "\n",
    "def process_files(path):\n",
    "    num_files=0\n",
    "    for entry in os.scandir(path):\n",
    "        if entry.is_dir():\n",
    "            process_files(entry.path)\n",
    "            continue\n",
    "        # Assumption: each directory either contains files or subdirectories.\n",
    "        # If it doesn't contain subdirectories, we only process its files if it is part of the training set.\n",
    "        if not remove_prefix(path, INPUT_DIR) in class_map:\n",
    "            print(\"Skipping %s...\" % path)\n",
    "            return\n",
    "        process_file(path, entry.name)\n",
    "        num_files += 1\n",
    "    print(\"Processed %d files in %s...\" % (num_files, path))\n",
    "\n",
    "\n",
    "process_files(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efd96e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 84 files in tmp/Data Collection (Images)/Lock + Key/Dorcas-final-2...\n",
      "Processed 70 files in tmp/Data Collection (Images)/Lock Picking/Dorcas-final-2...\n",
      "Processed 25 files in tmp/Data Collection (Images)/Empty/Dorcas-final-2...\n"
     ]
    }
   ],
   "source": [
    "class_map['/Lock + Key/Dorcas-final-2'] = 'lock-n-key'\n",
    "class_map['/Lock Picking/Dorcas-final-2'] = 'lock-pick'\n",
    "class_map['/Empty/Dorcas-final-2'] = 'empty'\n",
    "\n",
    "process_files(INPUT_DIR + '/Lock + Key/Dorcas-final-2')\n",
    "process_files(INPUT_DIR + '/Lock Picking/Dorcas-final-2')\n",
    "process_files(INPUT_DIR + '/Empty/Dorcas-final-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581dc2c",
   "metadata": {},
   "source": [
    "> Known issue: Some JPG images are, for whatever reason, rotated 90 degrees before being saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bb7de",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "For this set, we need to generate augmented data to increase the size pf the validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdd5d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 153 images belonging to 3 classes.\n",
      "[7, 9, 9]\n",
      "[19, 16, 15]\n",
      "[31, 24, 20]\n",
      "[43, 28, 29]\n",
      "[54, 34, 37]\n",
      "[67, 38, 45]\n",
      "[69, 39, 45]\n",
      "[75, 44, 52]\n",
      "[75, 48, 61]\n",
      "[75, 56, 70]\n",
      "[75, 62, 75]\n",
      "[75, 70, 75]\n",
      "[75, 75, 75]\n",
      "Processed tmp/Data Collection (Images)/validate.\n",
      "Found 118 images belonging to 3 classes.\n",
      "[13, 5, 7]\n",
      "[28, 10, 12]\n",
      "[39, 15, 21]\n",
      "[47, 22, 31]\n",
      "[56, 27, 35]\n",
      "[72, 30, 41]\n",
      "[75, 36, 52]\n",
      "[75, 43, 57]\n",
      "[75, 51, 63]\n",
      "[75, 54, 70]\n",
      "[75, 61, 75]\n",
      "[75, 66, 75]\n",
      "[75, 70, 75]\n",
      "[75, 75, 75]\n",
      "Processed tmp/Data Collection (Images)/test.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "import numpy as np\n",
    "\n",
    "VALIDATE_OUTPUT_DIR = 'datasets/validate'\n",
    "TEST_OUTPUT_DIR = 'datasets/test'\n",
    "\n",
    "num_images_required = 75\n",
    "batch_size = 25\n",
    "\n",
    "\n",
    "def create_generator(input_path):\n",
    "    # Include data augmentation techniques\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        brightness_range=(0.5, 1.5),\n",
    "        rotation_range=20,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    # Resize to the required input shape\n",
    "    generator = datagen.flow_from_directory(\n",
    "        input_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    return generator\n",
    "\n",
    "\n",
    "def process_files_for_augmentation(path, output_dir):\n",
    "    generator = create_generator(path)\n",
    "    classes = list(generator.class_indices.keys())\n",
    "\n",
    "    counts = [0, 0, 0]\n",
    "    # Generate batches of images to save to the given directory.\n",
    "    while np.min(counts) < num_images_required:\n",
    "        data, labels = next(generator)\n",
    "        num_items = np.shape(data)[0]\n",
    "\n",
    "        for i in range(num_items):\n",
    "            class_id = np.argmax(labels[i])\n",
    "            \n",
    "            if counts[class_id] >= num_images_required:\n",
    "                continue\n",
    "\n",
    "            image_data = data[i]\n",
    "            image = array_to_img(image_data)\n",
    "\n",
    "            save_path = \"%s/%s/%d.jpg\" % (output_dir, classes[class_id], counts[class_id])\n",
    "            image.save(save_path)\n",
    "\n",
    "            counts[class_id] += 1\n",
    "            \n",
    "        print(counts)\n",
    "\n",
    "    print(\"Processed %s.\" % path)\n",
    "    return\n",
    "\n",
    "\n",
    "process_files_for_augmentation(INPUT_DIR + \"/validate\", 'datasets/validate')\n",
    "process_files_for_augmentation(INPUT_DIR + \"/test\", 'datasets/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6e0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
