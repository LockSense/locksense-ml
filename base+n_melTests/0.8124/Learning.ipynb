{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "csvPath = Path.cwd()/'audioSampleFiles/metadata.csv'\n",
    "\n",
    "# Read metadata file\n",
    "df = pd.read_csv(csvPath)\n",
    "#df.head()\n",
    "#df.get('className')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import matplotlib.pyplot as plot\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "\n",
    "class AudioUtilities():\n",
    "    # ----------------------------\n",
    "    # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def open(fullPath):\n",
    "        signal, samplingRate = torchaudio.load(fullPath)\n",
    "        return (signal, samplingRate)\n",
    "    # ----------------------------\n",
    "    # Generate a Spectrogram\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def genSpectrogram(audio, numMels, numFft, hopLength):\n",
    "        signal,samplingRate = audio\n",
    "        maxDB = 80\n",
    "\n",
    "        # spec has shape [channel, n_mels, time], time = n_fft // 2 + 1\n",
    "        spectrogram = transforms.MelSpectrogram(samplingRate, n_fft=numFft, hop_length=hopLength, n_mels=numMels)(signal)\n",
    "        #print(spectrogram.shape)\n",
    "        \n",
    "        # Convert to decibels\n",
    "        spectrogram = transforms.AmplitudeToDB(top_db=maxDB)(spectrogram)\n",
    "        #print(spectrogram.shape)\n",
    "        return (spectrogram)\n",
    "    # ----------------------------\n",
    "    # For better generalisation, we will implement frequency and time masking at 1 mask at 10% of frequencies and of steps\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def augmentSpectrogram(spectrogram, maxMaskingRate, numFreqMasks, numTimeMasks):\n",
    "        _, numMels, numSteps = spectrogram.shape\n",
    "        maskValue = spectrogram.mean()\n",
    "        augmentedSpectrogram = spectrogram\n",
    "\n",
    "        frequencyMaskParameters = maxMaskingRate * numMels\n",
    "        for _ in range(numFreqMasks):\n",
    "            augmentedSpectrogram = transforms.FrequencyMasking(frequencyMaskParameters)(augmentedSpectrogram, maskValue)\n",
    "\n",
    "        timeMaskParameters = maxMaskingRate * numSteps\n",
    "        for _ in range(numTimeMasks):\n",
    "            augmentedSpectrogram = transforms.TimeMasking(timeMaskParameters)(augmentedSpectrogram, maskValue)\n",
    "\n",
    "        return augmentedSpectrogram\n",
    "    # ----------------------------\n",
    "    # Plot out spectrogram to view\n",
    "    # ----------------------------\n",
    "    def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "        fig, axs = plot.subplots(1, 1)\n",
    "        axs.set_title(title or 'Spectrogram (db)')\n",
    "        axs.set_ylabel(ylabel)\n",
    "        axs.set_xlabel('frame')\n",
    "        im = axs.imshow(spec, origin='lower', aspect=aspect)\n",
    "        if xmax:\n",
    "            axs.set_xlim((0, xmax))\n",
    "        fig.colorbar(im, ax=axs)\n",
    "        plot.show(block=False)\n",
    "    \n",
    "#audio = torchaudio.load(Path.cwd()/\"sampleFiles/readySamples/test1/test_0F.wav\")\n",
    "#signal, samplingRate = audio\n",
    "#print(samplingRate)\n",
    "#print(signal.shape)\n",
    "\n",
    "#spectrogram = AudioUtilities.genSpectrogram(audio, numMels=64, numFft=512, hopLength=None)\n",
    "#augmentedSpectrogram = AudioUtilities.augmentSpectrogram(spectrogram, maxMaskingRate=0.1, numFreqMasks=2, numTimeMasks=2)\n",
    "\n",
    "#AudioUtilities.plot_spectrogram(augmentedSpectrogram[0], title=\"Test0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Creating our custom Sound Dataset\n",
    "# ----------------------------\n",
    "class SoundDataSet(Dataset):\n",
    "    def __init__(self, df, dataPath):\n",
    "        self.df = df\n",
    "        self.dataPath = str(dataPath)\n",
    "                        \n",
    "    # ----------------------------\n",
    "    # Number of items in dataset\n",
    "    # ----------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.df)        \n",
    "        \n",
    "    # ----------------------------\n",
    "    # Get i'th item in dataset\n",
    "    # ----------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        # Absolute file path of the audio file\n",
    "        pathToAudio = self.dataPath + \"/\" + self.df.loc[idx, 'className'] + \"/\" + self.df.loc[idx, 'sampleName']\n",
    "        # Get the serial number of the class the file belongs to\n",
    "        classSerial = self.df.loc[idx, 'classSerial']\n",
    "        #print(classSerial)\n",
    "\n",
    "        audio = AudioUtilities.open(pathToAudio)\n",
    "        \n",
    "        spectrogram = AudioUtilities.genSpectrogram(audio, numMels=64, numFft=512, hopLength=None)\n",
    "        augmentedSpectrogram = AudioUtilities.augmentSpectrogram(spectrogram, maxMaskingRate=0.05, numFreqMasks=3, numTimeMasks=3)\n",
    "\n",
    "        return augmentedSpectrogram, classSerial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([533, 2, 64, 251])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import  DataLoader, random_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "dataPath = Path.cwd()/'audioSampleFiles/readySamples'\n",
    "dataset = SoundDataSet(df, dataPath)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "itemsCount = len(dataset)\n",
    "trainCount = round(itemsCount * 0.8)\n",
    "testCount = itemsCount - trainCount\n",
    "trainDataSet, testDataSet = random_split(dataset, [trainCount, testCount])\n",
    "#print(testCount)\n",
    "\n",
    "# Create training and validation data loaders\n",
    "trainDataLoader = DataLoader(trainDataSet, batch_size=trainDataSet.__len__(), shuffle=True)\n",
    "testDataLoader = DataLoader(testDataSet, batch_size=testDataSet.__len__(), shuffle=False)\n",
    "\n",
    "for inputs, labels in testDataLoader:\n",
    "    #print(\"test\")\n",
    "    #print(labels.shape)\n",
    "    print(inputs.shape)\n",
    "\n",
    "trainData, trainLabels = next(iter(trainDataLoader))\n",
    "testData, testLabels = next(iter(testDataLoader))\n",
    "\n",
    "#Conversion into class labels\n",
    "trainLabels = trainLabels.numpy()\n",
    "trainLabels = to_categorical(trainLabels, num_classes = 3)\n",
    "testLabels = testLabels.numpy()\n",
    "testLabels = to_categorical(testLabels, num_classes = 3)\n",
    "\n",
    "#Transposing the data\n",
    "trainData = trainData.numpy()\n",
    "trainData = np.transpose(trainData,(0,2,3,1))\n",
    "testData = testData.numpy()\n",
    "testData = np.transpose(testData,(0,2,3,1))\n",
    "\n",
    "trainMean = np.mean(trainData)\n",
    "trainStd = np.std(trainData)\n",
    "trainData = (trainData - trainMean) / trainData\n",
    "\n",
    "testMean = np.mean(testData)\n",
    "testStd = np.std(testData)\n",
    "testData = (testData - testMean) / testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 32, 126, 32)       1632      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 16, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 32, 64)         51264     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 4, 16, 128)        204928    \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 2, 8, 256)         819456    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 2,129,955\n",
      "Trainable params: 2,129,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load_model loads a model from a hd5 file.\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D\n",
    "import os\n",
    "\n",
    "MODEL_NAME = 'audio-cnn.hd5'\n",
    "\n",
    "def buildmodel(modelName):\n",
    "    if os.path.exists(modelName):\n",
    "        print(\"loading\")\n",
    "        model = load_model(modelName)                                                                                             \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(5,5), strides=2, activation='relu', input_shape=(64, 251, 2), padding='same'))\n",
    "\n",
    "        model.add(AveragePooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "        model.add(Conv2D(64, kernel_size=(5,5), strides=2, activation='relu', padding='same'))\n",
    "        model.add(Conv2D(128, kernel_size=(5,5), strides=2, activation='relu', padding='same'))\n",
    "        model.add(Conv2D(256, kernel_size=(5,5), strides=2, activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "                \n",
    "test = buildmodel(MODEL_NAME)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train(model, trainData, trainLabels, epochs, testData, testLabels, modelName):\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    savemodel = ModelCheckpoint(modelName)\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10)\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "\n",
    "    model.fit(\n",
    "        x=trainData, y=trainLabels, batch_size=128,\n",
    "        validation_data=(testData, testLabels), \n",
    "        shuffle=True, epochs=trainData.shape[0]//128, callbacks=[savemodel, stopmodel])\n",
    "\n",
    "    print(\"Done. Now evaluating.\")\n",
    "    loss, acc = model.evaluate(x=testData, y=testLabels)\n",
    "    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(trainData.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Epoch 1/16\n",
      "17/17 [==============================] - 10s 539ms/step - loss: 1.2588 - accuracy: 0.6598 - val_loss: 0.9785 - val_accuracy: 0.6735\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 2/16\n",
      "17/17 [==============================] - 9s 543ms/step - loss: 0.8709 - accuracy: 0.6828 - val_loss: 0.8739 - val_accuracy: 0.6735\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 3/16\n",
      "17/17 [==============================] - 8s 504ms/step - loss: 0.7382 - accuracy: 0.6893 - val_loss: 0.8697 - val_accuracy: 0.6773\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 4/16\n",
      "17/17 [==============================] - 9s 530ms/step - loss: 0.7317 - accuracy: 0.6959 - val_loss: 0.9457 - val_accuracy: 0.6735\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 5/16\n",
      "17/17 [==============================] - 9s 504ms/step - loss: 0.7169 - accuracy: 0.6973 - val_loss: 1.3826 - val_accuracy: 0.6623\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 6/16\n",
      "17/17 [==============================] - 9s 522ms/step - loss: 0.5589 - accuracy: 0.7755 - val_loss: 1.1387 - val_accuracy: 0.6904\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 7/16\n",
      "17/17 [==============================] - 9s 519ms/step - loss: 0.4562 - accuracy: 0.8004 - val_loss: 1.1318 - val_accuracy: 0.7580\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 8/16\n",
      "17/17 [==============================] - 9s 505ms/step - loss: 0.4802 - accuracy: 0.8238 - val_loss: 0.9279 - val_accuracy: 0.7711\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 9/16\n",
      "17/17 [==============================] - 9s 510ms/step - loss: 0.3046 - accuracy: 0.8847 - val_loss: 1.0471 - val_accuracy: 0.8161\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 10/16\n",
      "17/17 [==============================] - 9s 504ms/step - loss: 0.3580 - accuracy: 0.8693 - val_loss: 1.1748 - val_accuracy: 0.7974\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 11/16\n",
      "17/17 [==============================] - 9s 511ms/step - loss: 0.3425 - accuracy: 0.8688 - val_loss: 0.8769 - val_accuracy: 0.8274\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 12/16\n",
      "17/17 [==============================] - 9s 512ms/step - loss: 0.1895 - accuracy: 0.9367 - val_loss: 0.8120 - val_accuracy: 0.8443\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 13/16\n",
      "17/17 [==============================] - 9s 518ms/step - loss: 0.1408 - accuracy: 0.9470 - val_loss: 1.1183 - val_accuracy: 0.8143\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 14/16\n",
      "17/17 [==============================] - 8s 500ms/step - loss: 0.1772 - accuracy: 0.9391 - val_loss: 1.6299 - val_accuracy: 0.8518\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 15/16\n",
      "17/17 [==============================] - 9s 517ms/step - loss: 0.1134 - accuracy: 0.9658 - val_loss: 1.6861 - val_accuracy: 0.8537\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Epoch 16/16\n",
      "17/17 [==============================] - 8s 493ms/step - loss: 0.1465 - accuracy: 0.9499 - val_loss: 1.2948 - val_accuracy: 0.8124\n",
      "INFO:tensorflow:Assets written to: audio-cnn.hd5/assets\n",
      "Done. Now evaluating.\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 1.2948 - accuracy: 0.8124\n",
      "Test accuracy: 0.81, loss: 1.29\n"
     ]
    }
   ],
   "source": [
    "model = buildmodel(MODEL_NAME)\n",
    "train(model, trainData, trainLabels, 5, testData, testLabels, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
